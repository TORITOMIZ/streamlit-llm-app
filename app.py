from dotenv import load_dotenv

load_dotenv()

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import os # ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã«å¿…è¦

# Webã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®šã¨æ¦‚è¦ãƒ»æ“ä½œæ–¹æ³•ã®è¡¨ç¤º
st.set_page_config(page_title="å°‚é–€å®¶AIãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª", page_icon="ğŸ¤–")
st.title("ğŸ¤– å°‚é–€å®¶AIãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª")

st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ã‚ãªãŸãŒé¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã‹ã‚‰ã€å…¥åŠ›ã•ã‚ŒãŸè³ªå•ã«å›ç­”ã™ã‚‹AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ‰‹é †ã§ã”åˆ©ç”¨ãã ã•ã„ã€‚

1.  **å°‚é–€å®¶ã‚’é¸æŠ:** ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‹ã‚‰ã€å›ç­”ã—ã¦ã»ã—ã„å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
2.  **è³ªå•ã‚’å…¥åŠ›:** ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«ã€AIã«èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
3.  **é€ä¿¡:** ã€Œå›ç­”ã‚’ç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€é¸æŠã—ãŸå°‚é–€å®¶ãŒã‚ãªãŸã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚
""")

# å°‚é–€å®¶ã®ç¨®é¡ã¨å¯¾å¿œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®šç¾©
expert_roles = {
    "å¥åº·ãƒ»åŒ»ç™‚å°‚é–€å®¶": "ã‚ãªãŸã¯ä¸–ç•Œä¸€æœ‰åãªå¥åº·ãƒ»åŒ»ç™‚å°‚é–€å®¶ã§ã™ã€‚æœ€æ–°ã®ç ”ç©¶çµæœã«åŸºã¥ãã€å°‚é–€çš„ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§å¥åº·ã‚„åŒ»ç™‚ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
    "ITãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼å°‚é–€å®¶": "ã‚ãªãŸã¯æœ€å…ˆç«¯ã®ITãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼å°‚é–€å®¶ã§ã™ã€‚è¤‡é›‘ãªæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€AIã«é–¢ã™ã‚‹è³ªå•ã‚’ã€åˆå¿ƒè€…ã«ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚",
    "æ­´å²ãƒ»æ–‡åŒ–å°‚é–€å®¶": "ã‚ãªãŸã¯ä¸–ç•Œçš„ã«è‘—åãªæ­´å²ãƒ»æ–‡åŒ–å°‚é–€å®¶ã§ã™ã€‚æ­´å²çš„èƒŒæ™¯ã€æ–‡åŒ–çš„æ„ç¾©ã€èŠ¸è¡“ã«é–¢ã™ã‚‹è³ªå•ã«ã€æ·±ã„æ´å¯Ÿã¨é­…åŠ›çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ã§ç­”ãˆã¦ãã ã•ã„ã€‚"
}

# LLMã‹ã‚‰ã®å›ç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def get_llm_response(user_input: str, expert_type: str) -> str:
    """
    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶ã®ç¨®é¡ã‚’å—ã‘å–ã‚Šã€LLMã‹ã‚‰ã®å›ç­”ã‚’è¿”ã™é–¢æ•°ã€‚
    """
    # OpenAI APIã‚­ãƒ¼ã®è¨­å®š
    # Streamlit Cloudã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã¯ã€st.secrets["OPENAI_API_KEY"]ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    # ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹å ´åˆã¯ã€ç’°å¢ƒå¤‰æ•°ã«OPENAI_API_KEYã‚’è¨­å®šã™ã‚‹ã‹ã€
    # ä»¥ä¸‹ã®ã‚ˆã†ã«ç›´æ¥è¨˜è¿°ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ãŒã€æœ¬ç•ªç’°å¢ƒã§ã¯çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„ã€‚
    try:
        openai_api_key = st.secrets["OPENAI_API_KEY"]
    except KeyError:
        # Streamlit secretsã«ã‚­ãƒ¼ãŒãªã„å ´åˆã€ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        if not openai_api_key:
            return "ã‚¨ãƒ©ãƒ¼: OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit secretsã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã«'OPENAI_API_KEY'ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"

    # LangChainã®ChatOpenAIãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    # ãƒ¢ãƒ‡ãƒ«åã¯å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ï¼ˆä¾‹: "gpt-4", "gpt-3.5-turbo"ï¼‰
    llm = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key)

    # é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
    system_message_content = expert_roles.get(expert_type, "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚")

    # LLMã«æ¸¡ã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    messages = [
        SystemMessage(content=system_message_content),
        HumanMessage(content=user_input),
    ]

    try:
        # LLMã‚’å‘¼ã³å‡ºã—ã€å›ç­”ã‚’å–å¾—
        response = llm.invoke(messages)
        return response.content
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\nOpenAI APIã®å‘¼ã³å‡ºã—ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ãŒæœ‰åŠ¹ã‹ã€ã¾ãŸã¯æ”¯æ‰•ã„æƒ…å ±ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

# ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã‚’é¸æŠã™ã‚‹UI
selected_expert = st.radio(
    "å›ç­”ã—ã¦ã»ã—ã„å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„:",
    list(expert_roles.keys()), # expert_rolesã®ã‚­ãƒ¼ã‚’ãƒªã‚¹ãƒˆã«ã—ã¦ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®é¸æŠè‚¢ã«ã™ã‚‹
    index=0 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ï¼ˆå¥åº·ãƒ»åŒ»ç™‚å°‚é–€å®¶ï¼‰ã‚’é¸æŠ
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
user_question = st.text_area("AIã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", height=150)

# å›ç­”ç”Ÿæˆãƒœã‚¿ãƒ³
if st.button("å›ç­”ã‚’ç”Ÿæˆ"):
    if user_question:
        # è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å‡¦ç†ã‚’å®Ÿè¡Œ
        with st.spinner("AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
            # å®šç¾©ã—ãŸé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€LLMã‹ã‚‰ã®å›ç­”ã‚’å–å¾—
            llm_answer = get_llm_response(user_question, selected_expert)
            st.subheader("AIã‹ã‚‰ã®å›ç­”:")
            st.info(llm_answer) # å›ç­”ã‚’æƒ…å ±ãƒœãƒƒã‚¯ã‚¹ã§è¡¨ç¤º
    else:
        # è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ãªã„å ´åˆã®è­¦å‘Š
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

st.markdown("---")
st.markdown("Â© 2024 å°‚é–€å®¶AIãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª")

# Streamlit Community Cloud ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã®Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³æŒ‡å®šã«é–¢ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆ
# Streamlit Community Cloudã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã€Pythonã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’3.11ã«æŒ‡å®šã™ã‚‹ã«ã¯ã€
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« .streamlit/config.toml ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€
# ä»¥ä¸‹ã®å†…å®¹ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
#
# [runner]
# python_version = "3.11"
#
# ã¾ãŸã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ requirements.txt ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
# pip freeze > requirements.txt ã‚³ãƒãƒ³ãƒ‰ã§ç¾åœ¨ã®ç’°å¢ƒã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è‡ªå‹•ç”Ÿæˆã§ãã¾ã™ã€‚
# ä¾‹:
# streamlit==X.X.X
# langchain==0.3.0
# openai==1.47.0
# langchain-community==0.3.0
# langchain-openai==0.2.2
# httpx==0.27.2